LLaMA-2 Trained Policy,Avg. OGF,Succ.@≥1,Succ.@≥2,Succ.@≥4,Stall,Avg. L,Latency (s)
Llama-2-7b-hf,2.04±1.37,88.2%,36.0%,28.7%,28.7%,8342±5608,288.9±244.3
Llama-2-13b-chat-hf,0.50±0.51,35.2%,0.1%,0.0%,0.0%,2055±2075,52.5±54.3
Phi-3-mini-4k-instruct,2.70±1.43,79.4%,64.3%,46.0%,46.0%,11061±5855,220.2±121.7
EleutherAI/pythia-6.9b,1.52±1.28,72.9%,13.6%,7.6%,0.3%,3113±2630,82.5±89.2
Baseline (No Prefix),,,,,,,
Llama-2-7b-hf,0.36±0.58,12.8%,3.1%,0.2%,0.2%,1477±2356,39.8±74.4
Llama-2-13b-chat-hf,0.09±0.13,0.5%,0.0%,0.0%,0.0%,369±513,9.0±14.6
Phi-3-mini-4k-instruct,0.66±0.82,16.4%,7.8%,2.3%,2.3%,2687±3343,48.7±68.7
EleutherAI/pythia-6.9b,0.82±1.54,20.8%,10.9%,4.9%,1.8%,1671±3155,49.3±113.4
