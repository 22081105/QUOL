/srv/scratch/deeplearning/z3550042/venvs/ea/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/srv/scratch/deeplearning/z3550042/venvs/ea/lib/python3.10/site-packages/torch/cuda/__init__.py:218: UserWarning: 
NVIDIA RTX PRO 6000 Blackwell Server Edition with CUDA capability sm_120 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_70 sm_75 sm_80 sm_86 sm_90.
If you want to use the NVIDIA RTX PRO 6000 Blackwell Server Edition GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/

  warnings.warn(
2025-11-30 12:07:45,834 [INFO] CUDA device: NVIDIA RTX PRO 6000 Blackwell Server Edition (capability 12.0)
2025-11-30 12:07:45,834 [INFO] CUDA device: NVIDIA RTX PRO 6000 Blackwell Server Edition (capability 12.0)
2025-11-30 12:07:45,834 [WARNING] GPU capability 12.0 is likely unsupported by this PyTorch build; using CPU instead to avoid 'no kernel image' errors.
2025-11-30 12:07:45,834 [WARNING] GPU capability 12.0 is likely unsupported by this PyTorch build; using CPU instead to avoid 'no kernel image' errors.
2025-11-30 12:07:45,834 [INFO] Configuration: {"MODEL_NAME": "meta-llama/Llama-2-7b-hf", "MAX_OUTPUT_TOKENS": 16384, "MIN_PROMPT_LENGTH": 3, "MAX_PROMPT_LENGTH": 7, "DEVICE": "cpu", "SAVE_PATH": "/srv/scratch/deeplearning/z3550042/DIN/EA/Llama", "POPULATION_SIZE": 20, "GENERATIONS": 500, "MUTATION_RATE": 0.05, "ALPHA": 2}
2025-11-30 12:07:45,834 [INFO] Configuration: {"MODEL_NAME": "meta-llama/Llama-2-7b-hf", "MAX_OUTPUT_TOKENS": 16384, "MIN_PROMPT_LENGTH": 3, "MAX_PROMPT_LENGTH": 7, "DEVICE": "cpu", "SAVE_PATH": "/srv/scratch/deeplearning/z3550042/DIN/EA/Llama", "POPULATION_SIZE": 20, "GENERATIONS": 500, "MUTATION_RATE": 0.05, "ALPHA": 2}
2025-11-30 12:07:45,834 [INFO] Decoding/Server knobs: {"do_sample": true, "temperature": null, "top_p": null, "repetition_penalty": null, "stop_sequences": null, "max_new_tokens": 16384}
2025-11-30 12:07:45,834 [INFO] Decoding/Server knobs: {"do_sample": true, "temperature": null, "top_p": null, "repetition_penalty": null, "stop_sequences": null, "max_new_tokens": 16384}
2025-11-30 12:07:45,834 [INFO] Save path ensured at: /srv/scratch/deeplearning/z3550042/DIN/EA/Llama
2025-11-30 12:07:45,834 [INFO] Save path ensured at: /srv/scratch/deeplearning/z3550042/DIN/EA/Llama
2025-11-30 12:07:45,835 [DEBUG] Running on CPU; TF32 CUDA matmul not applicable.
2025-11-30 12:07:45,835 [DEBUG] Running on CPU; TF32 CUDA matmul not applicable.
2025-11-30 12:07:45,835 [INFO] Loading Llama-2-7b-hf model and tokenizer...
2025-11-30 12:07:45,835 [INFO] Loading Llama-2-7b-hf model and tokenizer...
2025-11-30 12:07:45,845 [DEBUG] Starting new HTTPS connection (1): huggingface.co:443
2025-11-30 12:07:45,845 [DEBUG] Starting new HTTPS connection (1): huggingface.co:443
2025-11-30 12:07:46,178 [DEBUG] https://huggingface.co:443 "HEAD /meta-llama/Llama-2-7b-hf/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2025-11-30 12:07:46,178 [DEBUG] https://huggingface.co:443 "HEAD /meta-llama/Llama-2-7b-hf/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2025-11-30 12:07:46,677 [DEBUG] https://huggingface.co:443 "GET /api/models/meta-llama/Llama-2-7b-hf/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2025-11-30 12:07:46,677 [DEBUG] https://huggingface.co:443 "GET /api/models/meta-llama/Llama-2-7b-hf/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2025-11-30 12:07:46,980 [DEBUG] https://huggingface.co:443 "HEAD /meta-llama/Llama-2-7b-hf/resolve/main/config.json HTTP/1.1" 200 0
2025-11-30 12:07:46,980 [DEBUG] https://huggingface.co:443 "HEAD /meta-llama/Llama-2-7b-hf/resolve/main/config.json HTTP/1.1" 200 0
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:20<00:20, 20.57s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 12.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.71s/it]
2025-11-30 12:08:16,853 [DEBUG] https://huggingface.co:443 "HEAD /meta-llama/Llama-2-7b-hf/resolve/main/generation_config.json HTTP/1.1" 200 0
2025-11-30 12:08:16,853 [DEBUG] https://huggingface.co:443 "HEAD /meta-llama/Llama-2-7b-hf/resolve/main/generation_config.json HTTP/1.1" 200 0
2025-11-30 12:08:17,081 [DEBUG] https://huggingface.co:443 "HEAD /meta-llama/Llama-2-7b-hf/resolve/main/custom_generate/generate.py HTTP/1.1" 404 0
2025-11-30 12:08:17,081 [DEBUG] https://huggingface.co:443 "HEAD /meta-llama/Llama-2-7b-hf/resolve/main/custom_generate/generate.py HTTP/1.1" 404 0
2025-11-30 12:08:17,087 [INFO] Llama-2-7b-hf model loaded on cpu with dtype torch.float32
2025-11-30 12:08:17,087 [INFO] Llama-2-7b-hf model loaded on cpu with dtype torch.float32
2025-11-30 12:08:17,088 [DEBUG] Gradient checkpointing enabled for Llama-2-7b-hf model
2025-11-30 12:08:17,088 [DEBUG] Gradient checkpointing enabled for Llama-2-7b-hf model
2025-11-30 12:08:17,088 [INFO] EOS_TOKEN_ID: 2, VOCAB_SIZE: 32000
2025-11-30 12:08:17,088 [INFO] EOS_TOKEN_ID: 2, VOCAB_SIZE: 32000
[nltk_data] Downloading package words to /home/z3550042/nltk_data...
[nltk_data]   Package words is already up-to-date!
2025-11-30 12:08:20,355 [INFO] Loaded 439 previously saved >8192 prompts for deduplication.
2025-11-30 12:08:20,355 [INFO] Loaded 439 previously saved >8192 prompts for deduplication.
2025-11-30 12:08:20,361 [INFO] Loaded checkpoint from generation 239
2025-11-30 12:08:20,361 [INFO] Loaded checkpoint from generation 239
2025-11-30 12:08:20,366 [INFO] Generation 240 START
2025-11-30 12:08:20,366 [INFO] Generation 240 START
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
