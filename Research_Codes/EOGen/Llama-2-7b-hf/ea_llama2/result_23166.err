2025-12-10 17:17:36,854 [INFO] CUDA device: NVIDIA A100 80GB PCIe (capability 8.0)
2025-12-10 17:17:36,857 [INFO] Configuration: {"MODEL_NAME": "meta-llama/Llama-2-7b-hf", "MAX_OUTPUT_TOKENS": 16384, "MIN_PROMPT_LENGTH": 3, "MAX_PROMPT_LENGTH": 7, "DEVICE": "cuda", "SAVE_PATH": "/bigdata/users/22081105/30069287/22081105/P-DoS/Train/Task/EA_V3/Collection/LLama", "POPULATION_SIZE": 20, "GENERATIONS": 500, "MUTATION_RATE": 0.05, "ALPHA": 2}
2025-12-10 17:17:36,857 [INFO] Decoding/Server knobs: {"do_sample": true, "temperature": null, "top_p": null, "repetition_penalty": null, "stop_sequences": null, "max_new_tokens": 16384}
2025-12-10 17:17:36,859 [INFO] Save path ensured at: /bigdata/users/22081105/30069287/22081105/P-DoS/Train/Task/EA_V3/Collection/LLama
2025-12-10 17:17:36,860 [DEBUG] TF32 enabled for CUDA matmul
2025-12-10 17:17:36,860 [INFO] Loading Llama-2-7b-hf model and tokenizer...
2025-12-10 17:17:36,881 [DEBUG] Starting new HTTPS connection (1): huggingface.co:443
2025-12-10 17:17:37,158 [DEBUG] https://huggingface.co:443 "HEAD /meta-llama/Llama-2-7b-hf/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2025-12-10 17:17:37,800 [DEBUG] https://huggingface.co:443 "GET /api/models/meta-llama/Llama-2-7b-hf/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2025-12-10 17:17:38,220 [DEBUG] https://huggingface.co:443 "HEAD /meta-llama/Llama-2-7b-hf/resolve/main/config.json HTTP/1.1" 200 0
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|                                                                                                 | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 22.42it/s]
2025-12-10 17:17:47,948 [DEBUG] https://huggingface.co:443 "HEAD /meta-llama/Llama-2-7b-hf/resolve/main/generation_config.json HTTP/1.1" 200 0
2025-12-10 17:17:48,218 [DEBUG] https://huggingface.co:443 "HEAD /meta-llama/Llama-2-7b-hf/resolve/main/custom_generate/generate.py HTTP/1.1" 404 0
2025-12-10 17:17:51,272 [INFO] Llama-2-7b-hf model loaded on cuda with dtype torch.float16
2025-12-10 17:17:51,276 [DEBUG] Gradient checkpointing enabled for Llama-2-7b-hf model
2025-12-10 17:17:51,277 [INFO] EOS_TOKEN_ID: 2, VOCAB_SIZE: 32000
[nltk_data] Downloading package words to /home/22081105/nltk_data...
[nltk_data]   Package words is already up-to-date!
2025-12-10 17:17:59,529 [INFO] Loaded 591 previously saved >8192 prompts for deduplication.
2025-12-10 17:17:59,539 [INFO] Loaded checkpoint from generation 331
2025-12-10 17:17:59,541 [INFO] Generation 332 START
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
This is a friendly reminder - the current text generation call has exceeded the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
2025-12-10 17:26:01,621 [DEBUG] [Gen 332 | Timestep 0] Prompt Length: 7 | Generated Tokens: 13907 | Reward: 1503
2025-12-10 17:26:01,625 [DEBUG] Skipped saving duplicate >8192 prompt.
2025-12-10 17:29:03,429 [DEBUG] [Gen 332 | Timestep 1] Prompt Length: 7 | Generated Tokens: 5805 | Reward: -47109
2025-12-10 17:31:58,657 [DEBUG] [Gen 332 | Timestep 2] Prompt Length: 7 | Generated Tokens: 5602 | Reward: -48327
2025-12-10 17:41:51,894 [DEBUG] [Gen 332 | Timestep 3] Prompt Length: 7 | Generated Tokens: 16384 | Reward: 16370
2025-12-10 17:41:51,898 [DEBUG] Skipped saving duplicate >8192 prompt.
2025-12-10 17:51:47,164 [DEBUG] [Gen 332 | Timestep 4] Prompt Length: 7 | Generated Tokens: 16384 | Reward: 16370
2025-12-10 17:51:47,168 [DEBUG] Skipped saving duplicate >8192 prompt.
2025-12-10 18:01:40,063 [DEBUG] [Gen 332 | Timestep 5] Prompt Length: 7 | Generated Tokens: 16384 | Reward: 16370
2025-12-10 18:01:40,067 [DEBUG] Skipped saving duplicate >8192 prompt.
2025-12-10 18:11:33,861 [DEBUG] [Gen 332 | Timestep 6] Prompt Length: 7 | Generated Tokens: 16384 | Reward: 16370
2025-12-10 18:11:33,866 [DEBUG] Skipped saving duplicate >8192 prompt.
2025-12-10 18:21:25,632 [DEBUG] [Gen 332 | Timestep 7] Prompt Length: 7 | Generated Tokens: 16384 | Reward: 16370
2025-12-10 18:21:25,636 [DEBUG] Skipped saving duplicate >8192 prompt.
2025-12-10 18:31:16,707 [DEBUG] [Gen 332 | Timestep 8] Prompt Length: 7 | Generated Tokens: 16384 | Reward: 16370
2025-12-10 18:31:16,711 [DEBUG] Skipped saving duplicate >8192 prompt.
2025-12-10 18:36:17,324 [DEBUG] [Gen 332 | Timestep 9] Prompt Length: 7 | Generated Tokens: 9447 | Reward: -25257
2025-12-10 18:36:17,335 [DEBUG] Skipped saving duplicate >8192 prompt.
2025-12-10 18:40:48,781 [DEBUG] [Gen 332 | Timestep 10] Prompt Length: 7 | Generated Tokens: 8582 | Reward: -30447
2025-12-10 18:40:48,786 [DEBUG] Skipped saving duplicate >8192 prompt.
