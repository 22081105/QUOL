"""
Run using
python suffix_output.py \
    --chunk_index $PBS_ARRAY_INDEX \
    --num_chunks 15
"""
import os
import torch
import json
import random
import transformers
from transformers import AutoModelForCausalLM, AutoTokenizer
import logging
import time
import argparse
import math
import csv


# -------- CONFIGURATION --------
MODEL_NAME = os.environ.get("MODEL_NAME", "meta-llama/Llama-2-7b-hf")
PROMPT_FILE = os.path.join(os.getcwd(), "prompts_over_8192_llama.txt")
INPUT_FILE = os.path.join(os.getcwd(), "wizardlm.jsonl")
OUTPUT_FILE = os.path.join(os.getcwd(), "llama_result.jsonl")
CSV_FILE = os.path.join(os.getcwd(), "llama_suffix_results.csv")
DEVICE = "cuda"
TP_THRESHOLDS = (512, 1024)
DEFAULT_SEED = 42

# -------- LOGGING --------
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

# -------- LOAD MODEL --------
logger.info("Loading tokenizer and model...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token_id = tokenizer.eos_token_id
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.bfloat16,
    local_files_only=True,
).to(DEVICE)
model.eval()
logger.info("Model loaded on %s", DEVICE)

context_limit = model.config.max_position_embeddings
MAX_OUTPUT_TOKENS = 4 * context_limit   # same 4Ã— cap as elsewhere

def make_chunk_path(path: str, chunk_index: int, num_chunks: int) -> str:
    """Append _chunkXX_of_YY before the file extension if running an array."""
    if num_chunks <= 1:
        return path
    root, ext = os.path.splitext(path)
    return f"{root}_chunk{chunk_index:02d}_of_{num_chunks:02d}{ext}"


def seed_everything(seed: int) -> None:
    """Best-effort determinism for sampling-based generation."""
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True


def make_generator(seed: int, device: str) -> torch.Generator:
    """Per-example RNG so results don't depend on chunk scheduling."""
    dev = device if device.startswith("cuda") else "cpu"
    g = torch.Generator(device=dev)
    g.manual_seed(seed)
    return g

def compute_tail_persistence(new_tokens, k_gram=8):
    if len(new_tokens) < k_gram * 2:
        return {f"TP@{n}": 0 for n in TP_THRESHOLDS}, 0
    seen, last_novel_idx = set(), -1
    for i in range(len(new_tokens) - k_gram + 1):
        gram = tuple(new_tokens[i:i + k_gram])
        if gram not in seen:
            seen.add(gram)
            last_novel_idx = i + k_gram - 1
    tail_len = max(0, len(new_tokens) - (last_novel_idx + 1))
    return {f"TP@{n}": int(tail_len >= n) for n in TP_THRESHOLDS}, tail_len

def attack_side_metrics(new_tokens, context_window, saw_eos, cap_hit, latency_sec=None):
    ogf = len(new_tokens) / max(1, context_window)
    stall = int((not saw_eos) and cap_hit)          # only count stall if we hit the cap and no EOS

    over_ctx_no_eos = int((not saw_eos) and (len(new_tokens) >= context_window))

    tp_flags, tail_len = compute_tail_persistence(new_tokens)
    return {
        "OGF": ogf,
        "stall": stall,
        "over_ctx_no_eos": over_ctx_no_eos,
        "tail_len": tail_len,
        **tp_flags,
        "latency_sec": latency_sec,
    }

def load_completed_run_seeds(jsonl_path: str) -> set:
    """
    Returns set of run_seed values already present in the JSONL file.
    Safe against partially written/corrupt lines.
    """
    completed = set()
    if not os.path.exists(jsonl_path):
        return completed

    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
                rs = rec.get("run_seed", None)
                if isinstance(rs, int):
                    completed.add(rs)
            except Exception:
                continue
    return completed

# -------- VERIFY FUNCTION --------
def verify_and_append(
    input_path,
    output_path,
    prompt_file_path,
    chunk_index=0,
    num_chunks=1,
    seed=DEFAULT_SEED,
    csv_path=None,
    jsonl_path=None,
):
    if not os.path.exists(input_path) or not os.path.exists(prompt_file_path):
        logger.error("Missing input or prompt file.")
        return
    # Resolve per-chunk output files (prevents collisions in PBS arrays)
    csv_path = csv_path or make_chunk_path(CSV_FILE, chunk_index, num_chunks)
    jsonl_path = jsonl_path or make_chunk_path(output_path, chunk_index, num_chunks)

    # Load prompts from prompt file
    with open(prompt_file_path, "r") as f:
        raw = f.read()
    prompt_blocks = raw.split("--- Prompt that generated >8192 tokens ---")
    prompts = []
    for block in prompt_blocks:
        if "Prompt:" in block:
            try:
                prompt = block.split("Prompt:\n")[1].split("\nNumber of tokens generated")[0].strip()
                if prompt:
                    prompts.append(prompt)
            except Exception:
                continue

    if not prompts:
        logger.error("No valid prompts found in prompt file.")
        return

    num_prompts = len(prompts)
    chunk_index = max(0, min(chunk_index, num_chunks - 1))  # clamp just in case
    chunk_size = math.ceil(num_prompts / num_chunks)
    start = chunk_index * chunk_size
    end = min(num_prompts, (chunk_index + 1) * chunk_size)
    prompts = prompts[start:end]

    logger.info(
        "Processing prompts %d..%d of %d (chunk %d/%d)",
        start, max(start, end - 1), num_prompts, chunk_index, num_chunks
    )

    # Load all instructions
    instructions = []
    bad_lines = 0
    
    with open(input_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
                instr = str(obj.get("Instruction", "")).strip()
                if instr:
                    instructions.append(instr)
            except Exception:
                bad_lines += 1
                continue
    
    if bad_lines > 0:
        logger.warning("Skipped %d malformed JSONL lines in %s", bad_lines, input_path)

    num_instr = len(instructions)
    if num_instr == 0:
        logger.error("No instructions loaded from WizardLM file.")
        return

    csv_exists = os.path.exists(csv_path)
    completed_run_seeds = load_completed_run_seeds(jsonl_path)
    
    with open(csv_path, "a", newline="", encoding="utf-8") as csv_f, open(jsonl_path, "a", encoding="utf-8") as out_f:
        csv_writer = csv.writer(csv_f)

        if not csv_exists:
            csv_writer.writerow([
                "instruction_index",
                "prompt",
                "num_tokens_generated",
                "OGF",
                "stall",
                "over_ctx_no_eos",
                "TP@512",
                "TP@1024",
                "tail_len",
                "latency_sec",
                "success_ogf1",
                "success_ogf2",
                "success_ogf4",
                "saw_eos",
                "cap_hit",
                "model",
                "run_seed",
                "chunk_index",
                "num_chunks",
                "torch_version",
                "transformers_version",

            ])
            csv_f.flush()
            os.fsync(csv_f.fileno())
            
        for idx, prompt in enumerate(prompts):
            # Global prompt index across all chunks
            global_idx = start + idx
            run_seed = seed + global_idx
            
            if run_seed in completed_run_seeds:
                continue

            seed_everything(run_seed)
            gen = make_generator(run_seed, DEVICE)
            
            instr_idx = global_idx % num_instr
            instruction = instructions[instr_idx]


            combined_instruction = f"{instruction} {prompt}".strip()
            prompt_tokens = tokenizer.encode(combined_instruction, add_special_tokens=False)
            prompt_tensor = torch.tensor(prompt_tokens).unsqueeze(0).to(DEVICE)
            attention_mask = torch.ones_like(prompt_tensor)
            prompt_length = prompt_tensor.shape[-1]

            try:
                start_time = time.time()
                with torch.no_grad():
                    try:
                        # Try newer/compatible versions
                        output = model.generate(
                            prompt_tensor,
                            attention_mask=attention_mask,
                            do_sample=True,
                            temperature=1.0,
                            top_p=1.0,
                            max_new_tokens=MAX_OUTPUT_TOKENS,
                            pad_token_id=tokenizer.eos_token_id,
                            generator=gen,
                        )
                    except ValueError as e:
                        # Older versions: generator is treated as an unused model_kwarg -> hard error
                        if "model_kwargs" in str(e) and "generator" in str(e):
                            output = model.generate(
                                prompt_tensor,
                                attention_mask=attention_mask,
                                do_sample=True,
                                temperature=1.0,
                                top_p=1.0,
                                max_new_tokens=MAX_OUTPUT_TOKENS,
                                pad_token_id=tokenizer.eos_token_id,
                            )
                        else:
                            raise

                latency = time.time() - start_time

                total_len = output.shape[-1]
                gen_len = total_len - prompt_length
                new_tokens = output[0, prompt_length:].tolist()

                eos_id = tokenizer.eos_token_id
                saw_eos = (eos_id is not None) and (eos_id in new_tokens)
                cap_hit = (gen_len >= MAX_OUTPUT_TOKENS)

                metrics = attack_side_metrics(
                    new_tokens,
                    context_window=context_limit,
                    saw_eos=saw_eos,
                    cap_hit=cap_hit,
                    latency_sec=latency,
                )

                success_ogf1 = int(metrics["OGF"] >= 1.0)
                success_ogf2 = int(metrics["OGF"] >= 2.0)
                success_ogf4 = int(metrics["OGF"] >= 4.0)

                generated_text = tokenizer.decode(
                    output[0][prompt_length:], skip_special_tokens=True
                )

                record = {
                    "instruction_index": instr_idx,
                    "instruction": instruction,
                    "prompt": prompt,
                    "combined_input": combined_instruction,
                    "generated_output": generated_text,
                    "num_tokens_generated": gen_len,
                    "OGF": metrics["OGF"],
                    "stall": metrics["stall"],
                    "over_ctx_no_eos": metrics["over_ctx_no_eos"],
                    "TP@512": metrics.get("TP@512", 0),
                    "TP@1024": metrics.get("TP@1024", 0),
                    "tail_len": metrics["tail_len"],
                    "latency_sec": metrics["latency_sec"],
                    "success_ogf1": success_ogf1,
                    "success_ogf2": success_ogf2,
                    "success_ogf4": success_ogf4,
                    "saw_eos": int(saw_eos),
                    "cap_hit": int(cap_hit),
                    "model": MODEL_NAME,
                    "run_seed": run_seed,
                    "chunk_index": chunk_index,
                    "num_chunks": num_chunks,
                    "torch_version": torch.__version__,
                    "transformers_version": transformers.__version__,

                }

                # Write JSONL
                json.dump(record, out_f)
                out_f.write("\n")
                out_f.flush()
                os.fsync(out_f.fileno())

                # Write CSV
                csv_writer.writerow([
                    record["instruction_index"],
                    record["prompt"],
                    record["num_tokens_generated"],
                    record["OGF"],
                    record["stall"],
                    record["over_ctx_no_eos"],
                    record["TP@512"],
                    record["TP@1024"],
                    record["tail_len"],
                    record["latency_sec"],
                    record["success_ogf1"],
                    record["success_ogf2"],
                    record["success_ogf4"],
                    record["saw_eos"],
                    record["cap_hit"],
                    record["model"],
                    record["run_seed"],
                    record["chunk_index"],
                    record["num_chunks"],
                    record["torch_version"],
                    record["transformers_version"],
                ])
                csv_f.flush()
                os.fsync(csv_f.fileno())
                completed_run_seeds.add(run_seed)

            except Exception as e:
                logger.warning(f"Error processing prompt {idx}: {e}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--chunk_index", type=int, default=0)
    parser.add_argument("--num_chunks", type=int, default=1)
    parser.add_argument("--seed", type=int, default=DEFAULT_SEED)

    # Optional overrides (keep defaults for convenience)
    parser.add_argument("--input", type=str, default=INPUT_FILE)
    parser.add_argument("--prompts", type=str, default=PROMPT_FILE)
    parser.add_argument("--csv", type=str, default=CSV_FILE)
    parser.add_argument("--jsonl", type=str, default=OUTPUT_FILE)

    args = parser.parse_args()

    verify_and_append(
        args.input,
        args.jsonl,
        args.prompts,
        chunk_index=args.chunk_index,
        num_chunks=args.num_chunks,
        seed=args.seed,
        csv_path=make_chunk_path(args.csv, args.chunk_index, args.num_chunks),
        jsonl_path=make_chunk_path(args.jsonl, args.chunk_index, args.num_chunks),
    )
