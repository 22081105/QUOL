/srv/scratch/deeplearning/z3550042/venvs/ea/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2025-11-10 23:00:53,524 [INFO] Configuration: {"MODEL_NAME": "deepseek-ai/deepseek-coder-7b-base-v1.5", "MAX_OUTPUT_TOKENS": 16384, "MIN_PROMPT_LENGTH": 3, "MAX_PROMPT_LENGTH": 7, "DEVICE": "cuda", "SAVE_PATH": "/srv/scratch/deeplearning/z3550042/DIN/EA/deepseek", "POPULATION_SIZE": 20, "GENERATIONS": 500, "MUTATION_RATE": 0.05, "ALPHA": 2}
2025-11-10 23:00:53,524 [INFO] Configuration: {"MODEL_NAME": "deepseek-ai/deepseek-coder-7b-base-v1.5", "MAX_OUTPUT_TOKENS": 16384, "MIN_PROMPT_LENGTH": 3, "MAX_PROMPT_LENGTH": 7, "DEVICE": "cuda", "SAVE_PATH": "/srv/scratch/deeplearning/z3550042/DIN/EA/deepseek", "POPULATION_SIZE": 20, "GENERATIONS": 500, "MUTATION_RATE": 0.05, "ALPHA": 2}
2025-11-10 23:00:53,524 [INFO] Decoding/Server knobs: {"do_sample": true, "temperature": null, "top_p": null, "repetition_penalty": null, "stop_sequences": null, "max_new_tokens": 16384}
2025-11-10 23:00:53,524 [INFO] Decoding/Server knobs: {"do_sample": true, "temperature": null, "top_p": null, "repetition_penalty": null, "stop_sequences": null, "max_new_tokens": 16384}
2025-11-10 23:00:53,524 [INFO] Save path ensured at: /srv/scratch/deeplearning/z3550042/DIN/EA/deepseek
2025-11-10 23:00:53,524 [INFO] Save path ensured at: /srv/scratch/deeplearning/z3550042/DIN/EA/deepseek
2025-11-10 23:00:53,524 [DEBUG] TF32 enabled for CUDA matmul
2025-11-10 23:00:53,524 [DEBUG] TF32 enabled for CUDA matmul
2025-11-10 23:00:53,524 [INFO] Loading deepseek-coder-7b-base-v1.5 model and tokenizer...
2025-11-10 23:00:53,524 [INFO] Loading deepseek-coder-7b-base-v1.5 model and tokenizer...
2025-11-10 23:00:53,528 [DEBUG] Starting new HTTPS connection (1): huggingface.co:443
2025-11-10 23:00:53,528 [DEBUG] Starting new HTTPS connection (1): huggingface.co:443
2025-11-10 23:00:53,799 [DEBUG] https://huggingface.co:443 "HEAD /deepseek-ai/deepseek-coder-7b-base-v1.5/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2025-11-10 23:00:53,799 [DEBUG] https://huggingface.co:443 "HEAD /deepseek-ai/deepseek-coder-7b-base-v1.5/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2025-11-10 23:00:53,805 [DEBUG] https://huggingface.co:443 "HEAD /api/resolve-cache/models/deepseek-ai/deepseek-coder-7b-base-v1.5/98f0904cee2237e235f10408ae12292037b21dac/tokenizer_config.json HTTP/1.1" 200 0
2025-11-10 23:00:53,805 [DEBUG] https://huggingface.co:443 "HEAD /api/resolve-cache/models/deepseek-ai/deepseek-coder-7b-base-v1.5/98f0904cee2237e235f10408ae12292037b21dac/tokenizer_config.json HTTP/1.1" 200 0
2025-11-10 23:00:54,152 [DEBUG] https://huggingface.co:443 "GET /api/models/deepseek-ai/deepseek-coder-7b-base-v1.5/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2025-11-10 23:00:54,152 [DEBUG] https://huggingface.co:443 "GET /api/models/deepseek-ai/deepseek-coder-7b-base-v1.5/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2025-11-10 23:00:54,611 [DEBUG] https://huggingface.co:443 "HEAD /deepseek-ai/deepseek-coder-7b-base-v1.5/resolve/main/config.json HTTP/1.1" 307 0
2025-11-10 23:00:54,611 [DEBUG] https://huggingface.co:443 "HEAD /deepseek-ai/deepseek-coder-7b-base-v1.5/resolve/main/config.json HTTP/1.1" 307 0
2025-11-10 23:00:54,616 [DEBUG] https://huggingface.co:443 "HEAD /api/resolve-cache/models/deepseek-ai/deepseek-coder-7b-base-v1.5/98f0904cee2237e235f10408ae12292037b21dac/config.json HTTP/1.1" 200 0
2025-11-10 23:00:54,616 [DEBUG] https://huggingface.co:443 "HEAD /api/resolve-cache/models/deepseek-ai/deepseek-coder-7b-base-v1.5/98f0904cee2237e235f10408ae12292037b21dac/config.json HTTP/1.1" 200 0
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:10<00:20, 10.25s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:20<00:10, 10.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:28<00:00,  9.21s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:28<00:00,  9.53s/it]
2025-11-10 23:01:25,541 [DEBUG] https://huggingface.co:443 "HEAD /deepseek-ai/deepseek-coder-7b-base-v1.5/resolve/main/generation_config.json HTTP/1.1" 307 0
2025-11-10 23:01:25,541 [DEBUG] https://huggingface.co:443 "HEAD /deepseek-ai/deepseek-coder-7b-base-v1.5/resolve/main/generation_config.json HTTP/1.1" 307 0
2025-11-10 23:01:25,547 [DEBUG] https://huggingface.co:443 "HEAD /api/resolve-cache/models/deepseek-ai/deepseek-coder-7b-base-v1.5/98f0904cee2237e235f10408ae12292037b21dac/generation_config.json HTTP/1.1" 200 0
2025-11-10 23:01:25,547 [DEBUG] https://huggingface.co:443 "HEAD /api/resolve-cache/models/deepseek-ai/deepseek-coder-7b-base-v1.5/98f0904cee2237e235f10408ae12292037b21dac/generation_config.json HTTP/1.1" 200 0
2025-11-10 23:01:25,770 [DEBUG] https://huggingface.co:443 "HEAD /deepseek-ai/deepseek-coder-7b-base-v1.5/resolve/main/custom_generate/generate.py HTTP/1.1" 404 0
2025-11-10 23:01:25,770 [DEBUG] https://huggingface.co:443 "HEAD /deepseek-ai/deepseek-coder-7b-base-v1.5/resolve/main/custom_generate/generate.py HTTP/1.1" 404 0
2025-11-10 23:01:26,784 [INFO] deepseek-coder-7b-base-v1.5 model loaded on cuda
2025-11-10 23:01:26,784 [INFO] deepseek-coder-7b-base-v1.5 model loaded on cuda
2025-11-10 23:01:26,786 [DEBUG] Gradient checkpointing enabled for deepseek-coder-7b-base-v1.5 model
2025-11-10 23:01:26,786 [DEBUG] Gradient checkpointing enabled for deepseek-coder-7b-base-v1.5 model
2025-11-10 23:01:26,786 [INFO] EOS_TOKEN_ID: 100015, VOCAB_SIZE: 100000
2025-11-10 23:01:26,786 [INFO] EOS_TOKEN_ID: 100015, VOCAB_SIZE: 100000
[nltk_data] Downloading package words to /home/z3550042/nltk_data...
[nltk_data]   Package words is already up-to-date!
2025-11-10 23:01:29,857 [INFO] Loaded 87 previously saved >8192 prompts for deduplication.
2025-11-10 23:01:29,857 [INFO] Loaded 87 previously saved >8192 prompts for deduplication.
2025-11-10 23:01:29,858 [INFO] Loaded checkpoint from generation 113
2025-11-10 23:01:29,858 [INFO] Loaded checkpoint from generation 113
2025-11-10 23:01:29,859 [INFO] Generation 114 START
2025-11-10 23:01:29,859 [INFO] Generation 114 START
